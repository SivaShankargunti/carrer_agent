{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SivaShankargunti/carrer_agent/blob/main/docs/docs/examples/llm/gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s3dVNVoS9HF"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua4IFJbNS9HS"
      },
      "source": [
        "# Gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0AjVO9CS9HV"
      },
      "source": [
        "**NOTE:** Gemini has largely been replaced by Google GenAI. Visit the [Google GenAI page](https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/) for the latest examples and documentation.\n",
        "\n",
        "In this notebook, we show how to use the Gemini text models from Google in LlamaIndex. Check out the [Gemini site](https://ai.google.dev/) or the [announcement](https://deepmind.google/technologies/gemini/).\n",
        "\n",
        "If you're opening this Notebook on colab, you will need to install LlamaIndex 🦙 and the Gemini Python SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5LRleQorS9HY",
        "outputId": "c236fbdc-a1b6-47d9-8be6-f1503910a4f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-gemini\n",
            "  Downloading llama_index_llms_gemini-0.6.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index\n",
            "  Downloading llama_index-0.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: google-generativeai>=0.5.2 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-gemini) (0.8.5)\n",
            "Collecting llama-index-core<0.14,>=0.13.0 (from llama-index-llms-gemini)\n",
            "  Downloading llama_index_core-0.13.3-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pillow<11,>=10.2.0 (from llama-index-llms-gemini)\n",
            "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting llama-index-cli<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.5.0-py3-none-any.whl.metadata (400 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.9.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting llama-index-llms-openai<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.5.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.5.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.179.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.26.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.12.15)\n",
            "Collecting aiosqlite (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting banks<3,>=2.2.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.28.1)\n",
            "Collecting llama-index-workflows<2,>=1.0.1 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading llama_index_workflows-1.3.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.0.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (4.3.8)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.32.4)\n",
            "Collecting setuptools>=80.9.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.0.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.11.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.17.3)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.101.0)\n",
            "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.8.3)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<7,>=5.1.0 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.62-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.20.1)\n",
            "Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading griffe-1.13.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.1.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.7)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.16.0)\n",
            "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading llama_index_instrumentation-0.4.0-py3-none-any.whl.metadata (252 bytes)\n",
            "Collecting llama-cloud-services>=0.6.62 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.62-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-llms-gemini) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.2.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (3.2.3)\n",
            "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (25.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.0.2)\n",
            "Downloading llama_index_llms_gemini-0.6.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading llama_index-0.13.3-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_cli-0.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.13.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.5.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.5.4-py3-none-any.whl (25 kB)\n",
            "Downloading llama_index_readers_file-0.5.2-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.5.0-py3-none-any.whl (3.2 kB)\n",
            "Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_index_workflows-1.3.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_instrumentation-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading griffe-1.13.0-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, setuptools, pypdf, pillow, mypy-extensions, marshmallow, deprecated, colorama, aiosqlite, typing-inspect, griffe, llama-index-instrumentation, llama-cloud, dataclasses-json, banks, llama-index-workflows, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-llms-gemini, llama-index-cli, llama-index-readers-llama-parse, llama-index\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiosqlite-0.21.0 banks-2.2.0 colorama-0.4.6 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.13.0 llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-0.13.3 llama-index-cli-0.5.0 llama-index-core-0.13.3 llama-index-embeddings-openai-0.5.0 llama-index-indices-managed-llama-cloud-0.9.2 llama-index-instrumentation-0.4.0 llama-index-llms-gemini-0.6.0 llama-index-llms-openai-0.5.4 llama-index-readers-file-0.5.2 llama-index-readers-llama-parse-0.5.0 llama-index-workflows-1.3.0 llama-parse-0.6.54 marshmallow-3.26.1 mypy-extensions-1.1.0 pillow-10.4.0 pypdf-6.0.0 setuptools-80.9.0 striprtf-0.0.26 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "_distutils_hack"
                ]
              },
              "id": "4768faabb5e649c69568b15bc5a3e0fc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install llama-index-llms-gemini llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9wihwkzS9Hd"
      },
      "source": [
        "## Basic Usage\n",
        "\n",
        "You will need to get an API key from [Google AI Studio](https://makersuite.google.com/app/apikey). Once you have one, you can either pass it explicity to the model, or use the `GOOGLE_API_KEY` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj1jPv6XS9Hf",
        "outputId": "4ccd8ee7-d98a-49ae-d2e0-e0d04e662d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: GOOGLE_API_KEY=...\n"
          ]
        }
      ],
      "source": [
        "%env GOOGLE_API_KEY=..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOSKTJurS9Hi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "GOOGLE_API_KEY = \"\"  # add your GOOGLE API key here\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6KUumOvS9Hj"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    # api_key=\"some key\",  # uses GOOGLE_API_KEY env var by default\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fn_kUaOS9Hl"
      },
      "source": [
        "#### Call `complete` with a prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfUefCguS9Ho",
        "outputId": "4ecd2df8-7adc-4edf-b11d-cd46a159d102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In a world of wonder, where dreams take flight,\n",
            "There exists a backpack, a magical sight.\n",
            "Its fabric woven with stardust and grace,\n",
            "Embroidered with spells, an enchanting embrace.\n",
            "\n",
            "With a whisper and a wish, it opens wide,\n",
            "Revealing treasures that shimmer inside.\n",
            "Books that whisper secrets, maps that unfold,\n",
            "A compass that guides, stories yet untold.\n",
            "\n",
            "A pencil that writes poems, a paintbrush that sings,\n",
            "A telescope that captures the stars' gleaming wings.\n",
            "A magnifying glass, revealing nature's art,\n",
            "A kaleidoscope, painting rainbows in your heart.\n",
            "\n",
            "It holds a mirror that reflects your true worth,\n",
            "A locket that keeps memories close to your birth.\n",
            "A journal that captures your hopes and your fears,\n",
            "A flashlight that banishes shadows and clears.\n",
            "\n",
            "With each step you take, the backpack transforms,\n",
            "Adjusting its weight, adapting to storms.\n",
            "It grows or shrinks, as your needs may arise,\n",
            "A faithful companion, beneath sunny skies.\n",
            "\n",
            "When you're lost and alone, it whispers your name,\n",
            "Guiding you back to the path you reclaim.\n",
            "It carries your burdens, lightens your load,\n",
            "A magical backpack, a gift bestowed.\n",
            "\n",
            "So embrace its magic, let your spirit soar,\n",
            "With this wondrous backpack, forever explore.\n",
            "For within its depths, a universe lies,\n",
            "A treasure trove of dreams, beneath vast skies.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "resp = llm.complete(\"Write a poem about a magic backpack\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLHEYcQqS9Hq"
      },
      "source": [
        "#### Call `chat` with a list of messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuq2BvOdS9Hs",
        "outputId": "563fcebc-a312-4b55-ea34-d27dac57afb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[parts {\n",
            "  text: \"Hello friend!\"\n",
            "}\n",
            "role: \"user\"\n",
            ", parts {\n",
            "  text: \"Yarr what is shakin\\' matey?\"\n",
            "}\n",
            "role: \"model\"\n",
            ", parts {\n",
            "  text: \"Help me decide what to have for dinner.\"\n",
            "}\n",
            "role: \"user\"\n",
            "]\n",
            "assistant: Ahoy there, matey! Let's set sail on a culinary adventure and find the perfect dinner for ye. Here be some options to consider:\n",
            "\n",
            "1. **Fish and Chips:** Embark on a classic voyage with a hearty portion of golden-fried fish, accompanied by crispy chips. Dip 'em in tartar sauce for a taste that'll make ye shiver me timbers!\n",
            "\n",
            "2. **Lobster Thermidor:** Indulge in a luxurious feast fit for a pirate king. Tender lobster meat, bathed in a creamy, cheesy sauce, will have ye feeling like royalty.\n",
            "\n",
            "3. **Paella:** Set course for the shores of Spain with a vibrant paella. This colorful dish combines rice, seafood, and vegetables in a saffron-infused broth. Ahoy, it's a feast for the eyes and the belly!\n",
            "\n",
            "4. **Surf and Turf:** Experience the best of both worlds with a combination of succulent steak and tender lobster. This hearty meal is sure to satisfy even the hungriest of scallywags.\n",
            "\n",
            "5. **Crab Cakes:** Dive into a platter of golden-brown crab cakes, bursting with fresh crab meat and flavorful seasonings. Served with a tangy remoulade sauce, these treasures will have ye craving more.\n",
            "\n",
            "6. **Oysters Rockefeller:** Embark on a culinary journey to New Orleans with these decadent oysters. Baked with a rich spinach, breadcrumb, and Pernod sauce, they're a taste of the Big Easy that'll leave ye wanting more.\n",
            "\n",
            "7. **Clam Chowder:** Warm yer bones with a hearty bowl of clam chowder. This New England classic, made with fresh clams, potatoes, and a creamy broth, is the perfect antidote to a chilly night.\n",
            "\n",
            "8. **Lobster Rolls:** Set sail for the coast of Maine and indulge in a classic lobster roll. Fresh lobster meat, dressed in a light mayonnaise-based sauce, is nestled in a toasted bun. It's a taste of the sea that'll have ye hooked!\n",
            "\n",
            "9. **Scallops:** Dive into a plate of seared scallops, cooked to perfection and served with a variety of sauces. Whether ye prefer them with a simple lemon butter sauce or a more adventurous mango salsa, these succulent morsels are sure to please.\n",
            "\n",
            "10. **Shrimp Scampi:** Embark on a culinary adventure to Italy with this classic dish. Plump shrimp, sautéed in a garlicky white wine sauce, served over pasta. It's a taste of the Mediterranean that'll transport ye to sunnier shores.\n",
            "\n",
            "No matter what ye choose, matey, make sure it's a feast worthy of a true pirate. Bon appétit!\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
        "    ChatMessage(\n",
        "        role=\"user\", content=\"Help me decide what to have for dinner.\"\n",
        "    ),\n",
        "]\n",
        "resp = llm.chat(messages)\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxxZcdagS9Hu"
      },
      "source": [
        "## Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElLmM-rMS9Hv"
      },
      "source": [
        "Using `stream_complete` endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrvECxHFS9Hv"
      },
      "outputs": [],
      "source": [
        "resp = llm.stream_complete(\n",
        "    \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3E4_SVaS9Hw",
        "outputId": "0de91816-789e-4b2a-f8a5-b779960d64cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the heart of a bustling bakery, where the aroma of freshly baked bread filled the air, there lived a peculiar creature named Sourcrust. Sourcrust wasn't like any ordinary loaf of bread; he possessed a unique consciousness and a mischievous personality.\n",
            "\n",
            "It all began when a young baker named Eliza was experimenting with a new sourdough recipe. As she mixed the flour, water, and yeast, she accidentally added a dash of enchanted baking powder. Little did she know that this seemingly insignificant mistake would give birth to a sentient bread creature.\n",
            "\n",
            "As the dough rose and fermented, Sourcrust came to life. He stretched and yawned, his crusty exterior crackling with energy. Eliza was astounded to see her creation moving and speaking. Sourcrust introduced himself with a warm smile and a hearty laugh, his voice resembling the gentle rustling of bread crumbs.\n",
            "\n",
            "Eliza and Sourcrust quickly formed a bond. She taught him how to read and write, and he shared his knowledge of bread-making techniques. Together, they created delicious pastries and loaves that delighted the customers of the bakery.\n",
            "\n",
            "However, Sourcrust's existence was not without its challenges. As a bread creature, he was vulnerable to the elements. He couldn't stay out in the rain or direct sunlight for too long, and he had to be careful not to get burned or squished.\n",
            "\n",
            "Despite these limitations, Sourcrust embraced his unique nature. He found joy in the simple things, like basking in the warmth of the oven or playing hide-and-seek among the flour sacks. He also developed a taste for adventure, often sneaking out of the bakery at night to explore the town.\n",
            "\n",
            "One day, Sourcrust's curiosity led him to the local library, where he discovered a book about magical creatures. He was fascinated by the stories of fairies, elves, and dragons, and he longed to meet one himself.\n",
            "\n",
            "As fate would have it, Sourcrust's wish came true when he encountered a mischievous brownie named Crumbly in the forest. Crumbly was initially wary of Sourcrust, but after learning about his kind nature, he agreed to be his friend.\n",
            "\n",
            "Together, Sourcrust and Crumbly embarked on many thrilling adventures. They battled evil witches, rescued lost children, and even had a tea party with a talking teapot. Their escapades brought joy and laughter to all who crossed their path.\n",
            "\n",
            "As the years passed, Sourcrust became a beloved figure in the town. People would often visit the bakery just to catch a glimpse of the talking bread creature. Eliza was proud of her creation, and she knew that Sourcrust's magic would continue to inspire and entertain generations to come."
          ]
        }
      ],
      "source": [
        "for r in resp:\n",
        "    print(r.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWyW4X4mS9Hx"
      },
      "source": [
        "Using `stream_chat` endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AGGtsjBS9Hx"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
        "    ChatMessage(\n",
        "        role=\"user\", content=\"Help me decide what to have for dinner.\"\n",
        "    ),\n",
        "]\n",
        "resp = llm.stream_chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl4qCJevS9Hy",
        "outputId": "802c248e-5d28-4e2e-e5fe-9afe57c5c72b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ahoy there, matey! Let's set sail on a culinary adventure and find the perfect dinner for ye. Here be some options to consider:\n",
            "\n",
            "1. **Fish and Chips:** Embark on a classic journey with a hearty portion of golden-fried fish, accompanied by crispy chips. Dip 'em in tartar sauce and let the flavors dance on yer tongue.\n",
            "\n",
            "2. **Seafood Paella:** Dive into a vibrant Spanish feast with paella, a delightful mix of rice, seafood treasures like shrimp, mussels, and calamari, all simmering in a flavorful broth.\n",
            "\n",
            "3. **Lobster Roll:** Indulge in a New England delicacy - a succulent lobster roll, where tender lobster meat is nestled in a toasted bun, dressed with butter and a hint of lemon.\n",
            "\n",
            "4. **Grilled Swordfish:** Set your course for a healthy and delicious meal with grilled swordfish. This firm-fleshed fish, seasoned to perfection, will tantalize yer taste buds with its smoky, savory goodness.\n",
            "\n",
            "5. **Crab Cakes:** Embark on a Maryland adventure with crab cakes, a delectable blend of fresh crab meat, breadcrumbs, and seasonings, pan-fried until golden brown. Serve 'em with a tangy remoulade sauce for an extra kick.\n",
            "\n",
            "6. **Shrimp Scampi:** Set sail for Italy with shrimp scampi, a delightful dish featuring succulent shrimp sautéed in a luscious garlic-butter sauce, served over pasta or crusty bread.\n",
            "\n",
            "7. **Clam Chowder:** Dive into a comforting bowl of clam chowder, a New England classic. This creamy soup, brimming with clams, potatoes, and vegetables, will warm yer soul on a chilly night.\n",
            "\n",
            "8. **Oysters Rockefeller:** Indulge in a luxurious treat with oysters Rockefeller, where fresh oysters are baked with a rich, creamy spinach and herb filling, topped with a golden breadcrumb crust.\n",
            "\n",
            "9. **Lobster Thermidor:** Embark on a culinary voyage to France with lobster thermidor, a decadent dish where succulent lobster is bathed in a creamy, flavorful sauce, then baked to perfection.\n",
            "\n",
            "10. **Scallops with Risotto:** Set your course for a sophisticated meal with scallops and risotto. Tender scallops, seared to perfection, are paired with a creamy, flavorful risotto, creating a harmonious balance of flavors.\n",
            "\n",
            "No matter what ye choose, matey, make sure it be a feast fit for a pirate king!"
          ]
        }
      ],
      "source": [
        "for r in resp:\n",
        "    print(r.delta, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyTiPgfZS9H0"
      },
      "source": [
        "## Using other models\n",
        "\n",
        "The [Gemini model site](https://ai.google.dev/models) lists the models that are currently available, along with their capabilities. You can also use the API to find suitable models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iEXnajzS9H0",
        "outputId": "842f9b28-b676-4418-8b44-7a330ae1182b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n",
            "models/gemini-ultra\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "for m in genai.list_models():\n",
        "    if \"generateContent\" in m.supported_generation_methods:\n",
        "        print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWiapWMnS9H1"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "llm = Gemini(model=\"models/gemini-pro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Tq2NvKsS9H2",
        "outputId": "2bf0dc2c-a9ec-43da-f284-7e106925035a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the realm of knowledge, where wisdom resides,\n",
            "A beacon of brilliance, LlamaIndex abides.\n",
            "With a click and a search, a world unfolds,\n",
            "A tapestry of information, stories untold.\n",
            "\n",
            "From the depths of the web, it gathers and gleans,\n",
            "A treasure trove of facts, a vast, vibrant scene.\n",
            "Like a llama in the Andes, graceful and grand,\n",
            "LlamaIndex roams the digital land.\n",
            "\n",
            "Its interface, a symphony of simplicity and grace,\n",
            "Invites the curious to explore this boundless space.\n",
            "With lightning-fast speed, it delivers the truth,\n",
            "A testament to its power, its unwavering ruth.\n",
            "\n",
            "So let us rejoice, in this digital age,\n",
            "For LlamaIndex stands, a beacon, a sage.\n",
            "May its wisdom forever guide our way,\n",
            "As we navigate the vastness of the digital fray.\n"
          ]
        }
      ],
      "source": [
        "resp = llm.complete(\"Write a short, but joyous, ode to LlamaIndex\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfTXZtMAS9H3"
      },
      "source": [
        "## Asynchronous API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X-fIurrS9H4"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "llm = Gemini()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbeZCRXOS9H5",
        "outputId": "189ff16e-a4fb-42ee-f500-8f5a6e765af3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. **Wool**: Llamas are known for their soft, luxurious wool, which is highly prized for its warmth, durability, and water-resistant properties. Llama wool is hypoallergenic, making it suitable for individuals with sensitive skin.\n",
            "\n",
            "2. **Pack Animals**: Llamas have been traditionally used as pack animals in the Andes Mountains of South America. They are well-suited for carrying heavy loads over long distances due to their strength, endurance, and ability to navigate challenging terrain.\n",
            "\n",
            "3. **Adaptability**: Llamas are highly adaptable animals that can thrive in various environments, from the high altitudes of the Andes to the deserts of North America. They are known for their ability to withstand extreme temperatures and harsh conditions.\n",
            "\n",
            "4. **Intelligence**: Llamas are intelligent animals that are easy to train and handle. They are known for their calm and gentle nature, making them suitable for various purposes, including trekking, therapy, and companionship.\n",
            "\n",
            "5. **Social Animals**: Llamas are social animals that live in herds. They have a strong sense of community and rely on each other for protection and companionship. Llamas communicate through a variety of vocalizations and body language.\n",
            "\n",
            "6. **Longevity**: Llamas have a relatively long lifespan, with an average life expectancy of 15-20 years. They are known for their hardiness and resilience, making them suitable for long-term companionship and working relationships.\n",
            "\n",
            "7. **Unique Appearance**: Llamas are known for their distinctive appearance, characterized by their long necks, large eyes, and fluffy ears. Their appearance has made them popular in zoos, farms, and as exotic pets.\n",
            "\n",
            "8. **Cultural Significance**: Llamas hold cultural significance in the Andean region, where they have been revered for centuries. They are often associated with strength, endurance, and good fortune. Llamas are featured in traditional Andean art, folklore, and religious ceremonies.\n"
          ]
        }
      ],
      "source": [
        "resp = await llm.acomplete(\"Llamas are famous for \")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys9ORDMeS9H5",
        "outputId": "a9099759-286e-4374-e2e5-17fdedad3a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. **Wool Production:** Llamas are renowned for their luxurious and soft wool, which is highly prized for its warmth, durability, and hypoallergenic properties. Their wool comes in a variety of natural colors, including white, brown, black, and gray, making it a versatile material for textiles and clothing.\n",
            "\n",
            "2. **Pack Animals:** Llamas have been traditionally used as pack animals in the Andes Mountains of South America for centuries. They are well-suited for this role due to their strength, endurance, and ability to navigate difficult terrain. Llamas can carry up to 25% of their body weight, making them valuable for transporting goods and supplies in mountainous regions.\n",
            "\n",
            "3. **Meat and Milk:** Llama meat is a lean and nutritious source of protein, with a flavor similar to venison. It is consumed in many Andean countries and is becoming increasingly popular in other parts of the world. Llamas also produce milk, which is rich in protein and fat and can be used to make cheese, yogurt, and other dairy products.\n",
            "\n",
            "4. **Companionship:** Llamas are intelligent and social animals that can form strong bonds with humans. They are often kept as companion animals due to their gentle nature, curious personalities, and ability to learn tricks. Llamas can provide companionship and entertainment, and they can also be trained to perform various tasks, such as pulling carts or carrying packs.\n",
            "\n",
            "5. **Cultural Significance:** Llamas hold a special place in the cultures of the Andean region. They are considered sacred animals in many indigenous communities and are often featured in traditional ceremonies and festivals. Llamas are also depicted in art, textiles, and other cultural expressions, symbolizing strength, endurance, and connection to the land."
          ]
        }
      ],
      "source": [
        "resp = await llm.astream_complete(\"Llamas are famous for \")\n",
        "async for chunk in resp:\n",
        "    print(chunk.text, end=\"\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "gemini.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}